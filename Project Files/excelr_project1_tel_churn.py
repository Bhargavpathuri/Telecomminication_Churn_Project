# -*- coding: utf-8 -*-
"""ExcelR_Project1_tel_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aewzHBTNB_IAzoy_9lCveU-j1TQ1hMvF
"""

import pandas as pd
data = pd.read_csv('telecommunications_churn.csv')
# Generate summary statistics
summary_statistics = data.describe(include='all').transpose()
display(summary_statistics)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

df = pd.read_csv('telecommunications_churn.csv')
# Checking for Missing Values
missing_values = df.isnull().sum()
# Encoding Binary Categorical Variables
df['international_plan'] = df['international_plan'].astype('category')
df['voice_mail_plan'] = df['voice_mail_plan'].astype('category')
# Normalizing Numerical Columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = numerical_columns.drop('churn')  # Excluding target variable

scaler = MinMaxScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])
# Splitting the Dataset into Features and Target
X = df.drop('churn', axis=1)
y = df['churn']
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Displaying preprocessing results and sample scaled data
preprocessing_summary = {
    "Missing Values": missing_values,
    "Data Types After Encoding": df.dtypes,
    "Numerical Columns Scaled": numerical_columns.tolist(),
    "Train Set Shape": X_train.shape,
    "Test Set Shape": X_test.shape,
    "Sample of Scaled Data": df.head()
}


from IPython.display import display
display(df) # Display the dataframe

preprocessing_summary

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Reload the dataset after the environment reset
data = pd.read_csv('telecommunications_churn.csv')
# Basic dataset copy for visualization
df = data.copy()
# Ensure seaborn styling
sns.set(style="whitegrid")
# Function for displaying plots clearly
def show_plot(title):
    plt.title(title)
    plt.tight_layout()
    plt.show()

# 1. Distribution of Target Variable (Churn)
plt.figure(figsize=(6, 4))
sns.countplot(x='churn', data=df, palette='coolwarm')
show_plot("Distribution of Churn (Target Variable)")
# 2. Relationship between Churn and Customer Service Calls
plt.figure(figsize=(8, 6))
sns.boxplot(x='churn', y='customer_service_calls', data=df, palette='coolwarm')
show_plot("Customer Service Calls vs Churn")
# 3. Distribution of Total Charges
plt.figure(figsize=(6, 4))
sns.histplot(df['total_charge'], kde=True, bins=30, color='blue')
show_plot("Distribution of Total Charges")
# 4. Churn vs International Plan and Voicemail Plan
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.countplot(x='international_plan', hue='churn', data=df, palette='coolwarm')
show_plot("Churn vs International Plan")

plt.subplot(1, 2, 2)
sns.countplot(x='voice_mail_plan', hue='churn', data=df, palette='coolwarm')
show_plot("Churn vs Voice Mail Plan")
# 5. Correlation Heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
show_plot("Correlation Heatmap")
# 6. Day Minutes vs Day Charges with Churn Hue
plt.figure(figsize=(8, 6))
sns.scatterplot(x='day_mins', y='day_charge', hue='churn', data=df, palette='coolwarm')
show_plot("Day Minutes vs Day Charges with Churn")

"""### Observations from Exploratory Data Analysis (EDA)

1. **Target Variable (Churn Distribution)**:
   - Approximately 14.5% of customers churn, indicating a class imbalance that needs to be addressed during modeling.

2. **Customer Service Calls vs. Churn**:
   - Customers with a higher number of customer service calls are more likely to churn.
   - This suggests dissatisfaction may lead to churn, and proactive customer service improvements could reduce churn.

3. **Distribution of Total Charges**:
   - Total charges range widely, with a skewed distribution towards the lower end.
   - Higher charges might be related to customers who are more likely to churn.

4. **Churn vs. International Plan and Voicemail Plan**:
   - Customers with an international plan are more likely to churn.
   - Customers with a voicemail plan show no significant difference in churn likelihood compared to those without.

5. **Correlation Heatmap**:
   - **Strong correlations**:
     - `day_mins` and `day_charge` (expected due to pricing structure).
     - Similar relationships are seen for evening and night charges and their corresponding minutes.
   - Weak correlations with the target variable `churn`, indicating the need for feature engineering or advanced models to identify nonlinear patterns.

6. **Day Minutes vs. Day Charges with Churn**:
   - A clear linear relationship exists between minutes and charges, irrespective of churn.
   - However, churned customers might slightly cluster at higher values, requiring further investigation.


"""

# Feature selection(Identifying the most influential features that correlate with churn.)
import matplotlib.pyplot as plt
import seaborn as sns
# Check correlation between numerical features and churn
correlation_matrix = data.corr()
churn_correlation = correlation_matrix["churn"].sort_values(ascending=False)
# Visualize correlations with the target variable (churn)
plt.figure(figsize=(10, 6))
sns.barplot(x=churn_correlation.index, y=churn_correlation.values)
plt.title("Correlation of Features with Churn")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Correlation")
plt.xlabel("Features")
plt.tight_layout()
plt.show()
# Extract top positively and negatively correlated features
top_positive_features = churn_correlation[churn_correlation > 0.1].index.tolist()
top_negative_features = churn_correlation[churn_correlation < -0.1].index.tolist()

top_positive_features, top_negative_features

"""The analysis reveals the following key features correlated with churn:

### Positively Correlated Features (higher values increase churn likelihood):
1. **International Plan**: Customers with an international plan are more likely to churn.
2. **Total Charge**: Higher overall charges are associated with higher churn rates.
3. **Customer Service Calls**: Frequent calls to customer service may indicate dissatisfaction.
4. **Day Minutes & Day Charge**: Longer day calls and higher charges increase churn likelihood.

### Negatively Correlated Feature (lower values reduce churn likelihood):
- **Voice Mail Plan**: Customers with a voice mail plan are less likely to churn.


"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Encode categorical features
data_encoded = data.copy()
encoder = LabelEncoder()

# Encode 'voice_mail_plan' and 'international_plan' as they are binary categorical features
data_encoded['voice_mail_plan'] = encoder.fit_transform(data_encoded['voice_mail_plan'])
data_encoded['international_plan'] = encoder.fit_transform(data_encoded['international_plan'])

# Split into features (X) and target (y)
X = data_encoded.drop(columns=['churn'])
y = data_encoded['churn']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Standardize numerical features (optional but useful for some models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    # Train the model
    model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, "predict_proba") else None

    # Metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None

    # Store results
    results[name] = {
        "Classification Report": report,
        "ROC-AUC": roc_auc
    }

results_summary = {model: {"Accuracy": results[model]["Classification Report"]["accuracy"],
                           "ROC-AUC": results[model]["ROC-AUC"]}
                   for model in results}

display(pd.DataFrame(results_summary).T)

"""Based on the results "Random Forest" Model has more Accuracy."""

!pip install streamlit

import joblib

# Get the Random Forest model from the 'models' dictionary
random_forest_model = models["Random Forest"]  # Access using the key "Random Forest"

# Save the model
joblib.dump(random_forest_model, "random_forest_model.pkl")

# Save the scaler
joblib.dump(scaler, "scaler.pkl")

